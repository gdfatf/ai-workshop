from __future__ import annotations

from pathlib import Path
import os
import streamlit as st

from dotenv import load_dotenv

# LangChain core
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.documents import Document

# LangChain + Gemini
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings

# RAG / Vector DB
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma

# DOCX loader
from docx import Document as DocxDocument


# =========================
# Streamlit åŸºç¡€è®¾ç½®
# =========================
st.set_page_config(page_title="AI Workbench Â· Gemini", layout="wide")
st.title("ğŸ§° AI Workbench Â· 13 Agents + RAGï¼ˆGemini 3 Proï¼‰")


# =========================
# Secrets / Env è¯»å–
# =========================
load_dotenv()

def sget(key: str, default: str | None = None) -> str | None:
    """
    å…ˆè¯» Streamlit Cloud secretsï¼›
    æœ¬åœ°æ²¡æœ‰ secrets.toml æ—¶ï¼Œst.secrets ä¼šæŠ› FileNotFoundError -> å…œåº•è¯»ç¯å¢ƒå˜é‡
    """
    try:
        # st.secrets å­˜åœ¨ä½†æœ¬åœ°æ²¡æ–‡ä»¶æ—¶ï¼Œä¼šåœ¨è®¿é—®/åŒ…å«åˆ¤æ–­æ—¶è§¦å‘ FileNotFoundError
        if key in st.secrets:
            return str(st.secrets[key])
    except FileNotFoundError:
        pass
    except Exception:
        # æç«¯æƒ…å†µä¸‹ secrets è§£æå¼‚å¸¸ï¼Œä¹Ÿå…œåº• env
        pass

    return os.getenv(key, default)


GOOGLE_API_KEY = sget("GOOGLE_API_KEY")
GEMINI_MODEL = sget("GEMINI_MODEL", "gemini-3-pro-preview")
# æ¨èï¼šGemini embedding æ¨¡å‹åå¸¦ models/ å‰ç¼€
GEMINI_EMBEDDING_MODEL = sget("GEMINI_EMBEDDING_MODEL", "models/gemini-embedding-001")
APP_PASSWORD = sget("APP_PASSWORD", "")

if not GOOGLE_API_KEY:
    st.error("ç¼ºå°‘ GOOGLE_API_KEYï¼šè¯·åœ¨ Streamlit Cloud çš„ Secrets é‡Œé…ç½® GOOGLE_API_KEYã€‚")
    st.stop()


# =========================
# ç®€å•å¯†ç é—¨ï¼ˆå¯é€‰ï¼‰
# =========================
if APP_PASSWORD:
    if "authed" not in st.session_state:
        st.session_state.authed = False

    if not st.session_state.authed:
        st.subheader("ğŸ”’ è¯·è¾“å…¥è®¿é—®å¯†ç ")
        pwd = st.text_input("Password", type="password")
        if st.button("è¿›å…¥"):
            if pwd == APP_PASSWORD:
                st.session_state.authed = True
                st.rerun()
            else:
                st.error("å¯†ç ä¸æ­£ç¡®")
        st.stop()


# =========================
# è·¯å¾„ä¸ç›®å½•
# =========================
PROMPTS_DIR = Path("agents") / "prompts"
KB_DIR = Path("kb")
DB_DIR = Path(".chroma_db")  # Streamlit Cloud: å®¹å™¨å†…å¯ç”¨ï¼›é‡å¯å¯èƒ½ä¸¢å¤±

PROMPTS_DIR.mkdir(parents=True, exist_ok=True)
KB_DIR.mkdir(parents=True, exist_ok=True)
DB_DIR.mkdir(parents=True, exist_ok=True)


AGENTS = {
    "01 äººè®¾å®šä½ Agent": "agent_01.txt",
    "02 åŸåˆ›çµæ„Ÿè®¨è®º Agent": "agent_02.txt",
    "03 çˆ†æ¬¾å†…å®¹ç­–åˆ’ Agent": "agent_03.txt",
    "04 æ–‡æ¡ˆèµŒæ³¨å¼ºåŒ– Agent": "agent_04.txt",
    "05 æ–‡æ¡ˆå¼€å¤´å¼ºåŒ– Agent": "agent_05.txt",
    "06 å†…å®¹äººæ ¼é­…åŠ›å¼ºåŒ– Agent": "agent_06.txt",
    "07 æ–‡æ¡ˆç”¨æˆ·éœ€æ±‚å¼ºåŒ– Agent": "agent_07.txt",
    "08 çº¿ç´¢è½¬åŒ–ç±»å†…å®¹ç­–åˆ’ Agent": "agent_08.txt",
    "09 å†…å®¹å½±åƒè®¾è®¡ Agent": "agent_09.txt",
    "10 å¤§çº²ç»“æ„ç»„ç»‡ Agent": "agent_10.txt",
    "11 å†…å®¹éª¨æ¶æ­å»º Agent": "agent_11.txt",
    "12 æ•´ä½“æ–‡æ¡ˆæ”¹å†™ Agent": "agent_12.txt",
    "13 ä¸ªäººIPè´¦å·è¿è¥é—®é¢˜è¯Šæ–­ Agent": "agent_13.txt",
}


# =========================
# å·¥å…·å‡½æ•°
# =========================
def load_prompt(filename: str) -> str:
    path = PROMPTS_DIR / filename
    if not path.exists():
        return "You are a helpful assistant."
    txt = path.read_text(encoding="utf-8").strip()
    return txt or "You are a helpful assistant."


def load_docx_text(path: Path) -> str:
    doc = DocxDocument(str(path))
    parts = [p.text for p in doc.paragraphs if p.text and p.text.strip()]
    return "\n".join(parts)


def load_kb_documents(agent_id: str) -> list[Document]:
    """
    ä» kb/agent_XX/ è¯»å– docx/txt -> LangChain Document
    """
    folder = KB_DIR / agent_id
    folder.mkdir(parents=True, exist_ok=True)

    docs: list[Document] = []
    for p in folder.rglob("*"):
        if p.is_dir():
            continue

        suf = p.suffix.lower()
        if suf == ".txt":
            docs.extend(TextLoader(str(p), encoding="utf-8").load())
        elif suf == ".docx":
            text = load_docx_text(p)
            if text.strip():
                docs.append(Document(page_content=text, metadata={"source": str(p)}))
    return docs


def build_embeddings() -> GoogleGenerativeAIEmbeddings:
    """
    Gemini Embeddingï¼šä¼˜å…ˆç”¨ secrets/env æŒ‡å®šçš„ GEMINI_EMBEDDING_MODELï¼›
    å¦‚æœä¸å¯ç”¨ï¼Œåˆ™å†å°è¯•ä¸€ä¸ªå¸¸è§å¤‡é€‰ã€‚
    """
    candidates = [
        GEMINI_EMBEDDING_MODEL,          # é»˜è®¤ models/gemini-embedding-001
        "models/text-embedding-004",     # æŸäº›è´¦å·/åœ°åŒºå¯ç”¨ï¼›ä¸å¯ç”¨ä¼šæŠ›é”™
    ]

    last_err = None
    for m in candidates:
        try:
            emb = GoogleGenerativeAIEmbeddings(
                model=m,
                google_api_key=GOOGLE_API_KEY,
            )
            _ = emb.embed_query("ping")  # è§¦å‘ä¸€æ¬¡å°è¯·æ±‚æ ¡éªŒ
            return emb
        except Exception as e:
            last_err = e

    raise RuntimeError(
        "æ²¡æœ‰å¯ç”¨çš„ embedding æ¨¡å‹ï¼Œè¯·æ£€æŸ¥è´¦å·æƒé™/åœ°åŒº/ç‰ˆæœ¬ã€‚"
        f"æœ€åé”™è¯¯ï¼š{last_err}"
    )


@st.cache_resource(show_spinner=False)
def get_vectorstore(agent_id: str):
    """
    æ¯ä¸ª agent ä¸€ä¸ª Chroma ç´¢å¼•ï¼ˆæœ¬åœ°æŒä¹…åŒ–åˆ° .chroma_db/agent_idï¼‰
    """
    embeddings = build_embeddings()

    persist_dir = DB_DIR / agent_id
    persist_dir.mkdir(parents=True, exist_ok=True)

    vs = Chroma(
        collection_name=f"kb_{agent_id}",
        embedding_function=embeddings,
        persist_directory=str(persist_dir),
    )

    # ä¸ºç©ºåˆ™å†™å…¥
    try:
        existing = vs._collection.count()
    except Exception:
        existing = 0

    if existing == 0:
        raw_docs = load_kb_documents(agent_id)
        if raw_docs:
            splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=120)
            chunks = splitter.split_documents(raw_docs)
            vs.add_documents(chunks)
            vs.persist()

    return vs


def retrieve_context(agent_id: str, query: str, k: int = 4) -> str:
    vs = get_vectorstore(agent_id)
    docs = vs.similarity_search(query, k=k) if vs else []
    if not docs:
        return ""

    blocks = []
    for i, d in enumerate(docs, 1):
        src = d.metadata.get("source", "kb")
        blocks.append(f"[ç‰‡æ®µ{i} | æ¥æº: {src}]\n{d.page_content}")
    return "\n\n".join(blocks)


def build_llm(model_name: str, temperature: float) -> ChatGoogleGenerativeAI:
    return ChatGoogleGenerativeAI(
        model=model_name,
        temperature=temperature,
        google_api_key=GOOGLE_API_KEY,
    )


# =========================
# Sidebar UI
# =========================
with st.sidebar:
    st.header("è®¾ç½®")

    st.write("Gemini Key exists:", True)
    st.write("Default Gemini Model:", GEMINI_MODEL)
    st.write("Embedding Model:", GEMINI_EMBEDDING_MODEL)

    agent_name = st.selectbox("é€‰æ‹© Agent", list(AGENTS.keys()))

    model_options = [
        "gemini-3-pro-preview",
        "gemini-3-flash-preview",
        "gemini-1.5-pro",
        "gemini-1.5-flash",
    ]
    model_name = st.selectbox(
        "æ¨¡å‹",
        model_options,
        index=model_options.index(GEMINI_MODEL) if GEMINI_MODEL in model_options else 0,
    )

    temperature = st.slider("temperature", 0.0, 1.0, 0.3, 0.05)

    use_rag = st.toggle("å¯ç”¨ RAGï¼ˆä» kb æ£€ç´¢ï¼‰", value=True)
    topk = st.slider("æ£€ç´¢ TopK", 1, 8, 4, 1)

    if st.button("æ¸…ç©ºå½“å‰ Agent å¯¹è¯"):
        st.session_state.pop(f"chat::{agent_name}", None)
        st.rerun()

    st.divider()
    st.caption("ğŸ“Œ çŸ¥è¯†åº“ç›®å½•ï¼škb/agent_01 ~ kb/agent_13ï¼ˆdocx/txtï¼‰")
    st.caption("ğŸ“Œ Prompt ç›®å½•ï¼šagents/prompts/agent_01.txt ~ agent_13.txt")


# =========================
# ä¸»æµç¨‹
# =========================
agent_file = AGENTS[agent_name]
system_prompt = load_prompt(agent_file)
agent_id = agent_file.replace(".txt", "")  # agent_01 ... agent_13

llm = build_llm(model_name=model_name, temperature=temperature)

chat_key = f"chat::{agent_name}"
if chat_key not in st.session_state:
    st.session_state[chat_key] = []
chat = st.session_state[chat_key]

with st.expander("æŸ¥çœ‹å½“å‰ Agent çš„ System Promptï¼ˆåªè¯»ï¼‰", expanded=False):
    st.code(system_prompt)

for msg in chat:
    if isinstance(msg, HumanMessage):
        with st.chat_message("user"):
            st.markdown(msg.content)
    else:
        with st.chat_message("assistant"):
            st.markdown(msg.content)

user_text = st.chat_input(f"æ­£åœ¨ä½¿ç”¨ï¼š{agent_name}ï¼ˆå¯ç²˜è´´é•¿æ–‡æœ¬ï¼‰")

if user_text:
    chat.append(HumanMessage(content=user_text))
    with st.chat_message("user"):
        st.markdown(user_text)

    rag_context = ""
    if use_rag:
        try:
            rag_context = retrieve_context(agent_id, user_text, k=topk)
        except Exception as e:
            # embedding/å‘é‡åº“æŒ‚äº†ï¼šä¸è®©æ•´ç«™å´©
            st.warning(f"RAG æš‚ä¸å¯ç”¨ï¼Œå·²è‡ªåŠ¨è·³è¿‡ã€‚åŸå› ï¼š{e}")
            rag_context = ""

    sys = system_prompt
    if rag_context:
        sys = (
            system_prompt
            + "\n\nã€å¯å¼•ç”¨çŸ¥è¯†åº“ç‰‡æ®µã€‘\n"
            + rag_context
            + "\n\nè¦æ±‚ï¼šå¦‚æœå¼•ç”¨äº†ç‰‡æ®µï¼Œè¯·åœ¨å›ç­”ä¸­æ ‡æ³¨æ¥æºç‰‡æ®µç¼–å·ã€‚"
        )

    messages = [SystemMessage(content=sys)] + chat

    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­â€¦"):
          def extract_text(resp) -> str:
    # Gemini / LangChain å…¼å®¹å¤„ç†
    if isinstance(resp.content, str):
        return resp.content
    if isinstance(resp.content, list):
        return "".join(
            block.get("text", "")
            for block in resp.content
            if isinstance(block, dict)
        )
    return str(resp.content)


resp = llm.invoke(messages)
answer = extract_text(resp)

st.markdown(answer)
chat.append(AIMessage(content=answer))



# =========================
# è°ƒè¯•é¢æ¿
# =========================
with st.expander("ğŸ§ª è°ƒè¯•é¢æ¿", expanded=False):
    st.write("å½“å‰ Agentï¼š", agent_name)
    st.write("agent_idï¼š", agent_id)
    st.write("æ¨¡å‹ï¼š", model_name)
    st.write("RAGï¼š", use_rag, "TopK=", topk)
