from __future__ import annotations

from pathlib import Path
import os
from typing import Optional, List, Any, Dict

import streamlit as st
from dotenv import load_dotenv

# ========= LangChain / Providers =========
from langchain_anthropic import ChatAnthropic
from langchain_openai import OpenAIEmbeddings

# ========= LangChain core =========
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.documents import Document

# ========= RAG / Vector DB =========
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma

# ========= DOCX loader =========
from docx import Document as DocxDocument

# ========= Supabase =========
from supabase import create_client, Client


# =========================
# Streamlit åŸºç¡€è®¾ç½®
# =========================
st.set_page_config(page_title="AI Workbench Â· 4-Agent çŸ­è§†é¢‘åˆ›ä½œç³»ç»Ÿ", layout="wide")
st.title("ğŸ¬ AI Workbench Â· 4-Agent çŸ­è§†é¢‘å…¨é“¾è·¯åˆ›ä½œç³»ç»Ÿ")


# =========================
# Env/Secrets
# =========================
load_dotenv()


def sget(key: str, default: Optional[str] = None) -> Optional[str]:
    try:
        if hasattr(st, "secrets") and key in st.secrets:
            return str(st.secrets[key])
    except FileNotFoundError:
        pass
    return os.getenv(key, default)

ANTHROPIC_API_KEY = sget("ANTHROPIC_API_KEY")
LLM_MODEL_DEFAULT = sget("LLM_MODEL", "claude-opus-4-6")
OPENAI_API_KEY = sget("OPENAI_API_KEY")
APP_PASSWORD = sget("APP_PASSWORD", "")

OPENAI_EMBED_MODEL_DEFAULT = sget("OPENAI_EMBED_MODEL", "text-embedding-3-large")

SUPABASE_URL = sget("SUPABASE_URL")
SUPABASE_SERVICE_ROLE_KEY = sget("SUPABASE_SERVICE_ROLE_KEY")

if not ANTHROPIC_API_KEY:
    st.error("ç¼ºå°‘ ANTHROPIC_API_KEYï¼šè¯·åœ¨ Streamlit Secrets æˆ–æœ¬åœ° .env ä¸­é…ç½®ã€‚")
    st.stop()

if not OPENAI_API_KEY:
    st.error("ç¼ºå°‘ OPENAI_API_KEYï¼šè¯·åœ¨ Streamlit Secrets æˆ–æœ¬åœ° .env ä¸­é…ç½®ï¼ˆç”¨äº Embedding / RAGï¼‰ã€‚")
    st.stop()

if not SUPABASE_URL or not SUPABASE_SERVICE_ROLE_KEY:
    st.error("ç¼ºå°‘ SUPABASE_URL / SUPABASE_SERVICE_ROLE_KEYï¼šè¯·åœ¨ Streamlit Secrets é…ç½®ï¼ˆç”¨äºè®°å¿†å­˜å‚¨ï¼‰ã€‚")
    st.stop()


@st.cache_resource(show_spinner=False)
def get_supabase() -> Client:
    return create_client(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY)


sb = get_supabase()


# =========================
# ç®€å•å¯†ç é—¨ï¼ˆå¯é€‰ï¼‰
# =========================
if APP_PASSWORD:
    if "authed" not in st.session_state:
        st.session_state.authed = False

    if not st.session_state.authed:
        st.subheader("ğŸ”’ è¯·è¾“å…¥è®¿é—®å¯†ç ")
        pwd = st.text_input("Password", type="password", key="pwd_input")
        if st.button("è¿›å…¥", key="pwd_btn"):
            if pwd == APP_PASSWORD:
                st.session_state.authed = True
                st.rerun()
            else:
                st.error("å¯†ç ä¸æ­£ç¡®")
        st.stop()


# =========================
# è·¯å¾„ä¸ç›®å½•
# =========================
PROMPTS_DIR = Path("agents") / "prompts"
KB_DIR = Path("kb")
DB_DIR = Path("chroma_db")

PROMPTS_DIR.mkdir(parents=True, exist_ok=True)
KB_DIR.mkdir(parents=True, exist_ok=True)
DB_DIR.mkdir(parents=True, exist_ok=True)

# =====================================================
# âœ… æ–°ç‰ˆ 4-Agent å®šä¹‰ï¼ˆæ›¿ä»£åŸ 13-Agentï¼‰
# =====================================================
AGENTS = {
    "Aï½œè´¦å·å®šä½å¸ˆ Â· æˆ˜ç•¥ç½—ç›˜": "agent_a.txt",
    "Bï½œé€‰é¢˜æ¶æ„å¸ˆ Â· çˆ†æ¬¾å¼•æ“": "agent_b.txt",
    "Cï½œæ–‡æ¡ˆç‚¼é‡‘å¸ˆ Â· ç»“æ„å¤–ç§‘": "agent_c.txt",
    "Dï½œå½±åƒå¯¼æ¼” Â· æƒ…ç»ªå»ºç­‘å¸ˆ": "agent_d.txt",
}

# Agent æè¿°ï¼ˆæ˜¾ç¤ºåœ¨ä¾§è¾¹æ ï¼‰
AGENT_DESCRIPTIONS = {
    "Aï½œè´¦å·å®šä½å¸ˆ Â· æˆ˜ç•¥ç½—ç›˜": "å®šä½è¯Šæ–­ â†’ èµ›é“éªŒè¯ â†’ å˜ç°è®¾è®¡ â†’ äººè®¾é”»é€ ",
    "Bï½œé€‰é¢˜æ¶æ„å¸ˆ Â· çˆ†æ¬¾å¼•æ“": "çµæ„Ÿæ•è· â†’ é€‰é¢˜æ‰“ç£¨ â†’ æµé‡æ¼æ–— â†’ æ¶æ„é€‰æ‹©",
    "Cï½œæ–‡æ¡ˆç‚¼é‡‘å¸ˆ Â· ç»“æ„å¤–ç§‘": "ç»“æ„ä½“æ£€ â†’ é’©å­æ‰‹æœ¯ â†’ éª¨æ¶é‡æ’ â†’ åˆºç‚¹æ¤å…¥ â†’ è¯­è¨€é›•ç¢",
    "Dï½œå½±åƒå¯¼æ¼” Â· æƒ…ç»ªå»ºç­‘å¸ˆ": "æƒ…æ„Ÿå›¢å— â†’ é£æ ¼å®šè°ƒ â†’ åˆ†é•œè„šæœ¬ â†’ è¡¨æ¼”æŒ‡å¯¼",
}


# =========================
# Supabase Memory
# =========================
def db_load_chat(user_id: str, agent_id: str) -> List[Dict[str, str]]:
    try:
        resp = (
            sb.table("workbench_memory")
            .select("messages")
            .eq("user_id", user_id)
            .eq("agent_id", agent_id)
            .execute()
        )
        data = resp.data
        if not data:
            return []
        row = data[0] if isinstance(data, list) else data
        msgs = row.get("messages", [])
        return msgs if isinstance(msgs, list) else []
    except Exception:
        return []


def db_save_chat(user_id: str, agent_id: str, messages: List[Dict[str, str]]) -> None:
    try:
        sb.table("workbench_memory").upsert(
            {
                "user_id": user_id,
                "agent_id": agent_id,
                "messages": messages,
            }
        ).execute()
    except Exception:
        pass


def db_clear_chat(user_id: str, agent_id: str) -> None:
    db_save_chat(user_id, agent_id, [])


def lc_to_json(messages: List[Any]) -> List[Dict[str, str]]:
    out: List[Dict[str, str]] = []
    for m in messages:
        if isinstance(m, HumanMessage):
            out.append({"role": "user", "content": m.content})
        elif isinstance(m, AIMessage):
            out.append({"role": "assistant", "content": m.content})
        elif isinstance(m, SystemMessage):
            out.append({"role": "system", "content": m.content})
    return out


def json_to_lc(items: List[Dict[str, str]]) -> List[Any]:
    out: List[Any] = []
    for it in items:
        role = it.get("role")
        content = it.get("content", "")
        if role == "user":
            out.append(HumanMessage(content=content))
        elif role == "assistant":
            out.append(AIMessage(content=content))
        elif role == "system":
            out.append(SystemMessage(content=content))
    return out


# =========================
# å·¥å…·å‡½æ•°
# =========================
def load_prompt(filename: str) -> str:
    path = PROMPTS_DIR / filename
    if not path.exists():
        return "You are a helpful assistant."
    txt = path.read_text(encoding="utf-8").strip()
    return txt or "You are a helpful assistant."


def load_docx_text(path: Path) -> str:
    doc = DocxDocument(str(path))
    parts = [p.text for p in doc.paragraphs if p.text and p.text.strip()]
    return "\n".join(parts)


def load_kb_documents(agent_id: str) -> List[Document]:
    folder = KB_DIR / agent_id
    folder.mkdir(parents=True, exist_ok=True)

    docs: List[Document] = []
    for p in folder.rglob("*"):
        if p.is_dir():
            continue
        suf = p.suffix.lower()

        if suf in (".txt", ".md"):
            docs.extend(TextLoader(str(p), encoding="utf-8").load())

        elif suf == ".docx":
            text = load_docx_text(p)
            if text.strip():
                docs.append(Document(page_content=text, metadata={"source": str(p)}))

    return docs


def extract_text(resp) -> str:
    content = getattr(resp, "content", resp)
    if isinstance(content, str):
        return content
    if isinstance(content, list):
        out = []
        for block in content:
            if isinstance(block, dict):
                out.append(str(block.get("text", "")))
            else:
                out.append(str(block))
        return "".join(out)
    return str(content)


def build_llm(model_name: str, temperature: float) -> ChatAnthropic:
    return ChatAnthropic(
        model=model_name,
        temperature=temperature,
    )
    


def build_embeddings(model_name: str) -> OpenAIEmbeddings:
    return OpenAIEmbeddings(model=model_name, api_key=OPENAI_API_KEY)


@st.cache_resource(show_spinner=False)
def get_vectorstore(agent_id: str, embed_model: str):
    embeddings = build_embeddings(embed_model)

    persist_dir = DB_DIR / agent_id
    persist_dir.mkdir(parents=True, exist_ok=True)

    safe_model = embed_model.replace("/", "_").replace(":", "_")
    collection_name = f"kb_{agent_id}__{safe_model}"

    vs = Chroma(
        collection_name=collection_name,
        embedding_function=embeddings,
        persist_directory=str(persist_dir),
    )

    try:
        existing = vs._collection.count()
    except Exception:
        existing = 0

    if existing == 0:
        raw_docs = load_kb_documents(agent_id)
        if raw_docs:
            # âœ… ä½¿ç”¨ Markdown æ„ŸçŸ¥çš„åˆ†éš”ç¬¦ï¼Œæ›´å¥½åœ°åˆ‡åˆ†ç»“æ„åŒ– KB
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=1200,
                chunk_overlap=150,
                separators=["\n## ", "\n### ", "\n#### ", "\n---", "\n\n", "\n", " "],
            )
            chunks = splitter.split_documents(raw_docs)
            vs.add_documents(chunks)
            vs.persist()

    return vs


def retrieve_context(agent_id: str, query: str, k: int, embed_model: str) -> str:
    vs = get_vectorstore(agent_id, embed_model=embed_model)
    docs = vs.similarity_search(query, k=k)
    if not docs:
        return ""

    blocks = []
    for i, d in enumerate(docs, 1):
        src = d.metadata.get("source", "kb")
        blocks.append(f"[ç‰‡æ®µ{i} | æ¥æº: {src}]\n{d.page_content}")
    return "\n\n".join(blocks)


# =========================
# Sidebar UI
# =========================
with st.sidebar:
    st.header("âš™ï¸ è®¾ç½®")

    st.subheader("ç”¨æˆ·èº«ä»½ï¼ˆç”¨äºè®°å¿†éš”ç¦»ï¼‰")
    user_id = st.text_input("ä½ çš„ç”¨æˆ·IDï¼ˆæ¯äººå›ºå®šä¸€ä¸ªï¼‰", key="user_id_input")

    if not user_id or not user_id.strip():
        st.warning("è¯·è¾“å…¥ç”¨æˆ·IDï¼ˆä¾‹å¦‚ï¼šå§“åæ‹¼éŸ³/ä»£å·ï¼‰ï¼Œç”¨äºä¿å­˜ä¸ªäººè®°å¿†ã€‚")
        st.stop()
    user_id = user_id.strip()

    st.divider()

    # âœ… Agent é€‰æ‹© + æè¿°
    agent_name = st.selectbox("é€‰æ‹© Agent", list(AGENTS.keys()), key="agent_select")

    desc = AGENT_DESCRIPTIONS.get(agent_name, "")
    if desc:
        st.caption(f"ğŸ“‹ {desc}")

    st.divider()

claude_candidates = [
    "claude-opus-4-6",
    "claude-sonnet-4-5",
    "claude-haiku-3",
]

claude_default = (
    LLM_MODEL_DEFAULT
    if LLM_MODEL_DEFAULT in claude_candidates
    else claude_candidates[0]
)

model_name = st.selectbox(
    "LLMï¼ˆClaudeï¼‰",
    claude_candidates,
    index=claude_candidates.index(claude_default),
    key="llm_select",
)

embed_candidates = [
    "text-embedding-3-large",
    "text-embedding-3-small",
]
embed_default = (
    OPENAI_EMBED_MODEL_DEFAULT
    if OPENAI_EMBED_MODEL_DEFAULT in embed_candidates
    else embed_candidates[0]
)
embed_model = st.selectbox(
    "Embeddingï¼ˆOpenAIï¼‰",
    embed_candidates,
    index=embed_candidates.index(embed_default),
    key="embed_select",
)
    temperature = st.slider("temperature", 0.0, 1.0, 0.3, 0.05, key="temp_slider")

    use_rag = st.toggle("å¯ç”¨ RAGï¼ˆä» KB æ£€ç´¢ï¼‰", value=True, key="rag_toggle")
    topk = st.slider("æ£€ç´¢ TopK", 1, 8, 4, 1, key="topk_slider")

    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå½“å‰ Agent å¯¹è¯", key="clear_chat_btn"):
        agent_file_tmp = AGENTS[agent_name]
        agent_id_tmp = agent_file_tmp.replace(".txt", "")
        db_clear_chat(user_id, agent_id_tmp)
        st.session_state.pop(f"chat::{user_id}::{agent_id_tmp}", None)
        st.rerun()

    st.divider()
    st.caption("ğŸ“‚ Prompt â†’ agents/prompts/agent_a~d.txt")
    st.caption("ğŸ“‚ KB â†’ kb/agent_a~d/ï¼ˆ.txt .md .docxï¼‰")

    st.divider()
    st.caption("ğŸ”„ åˆ›ä½œæµç¨‹ï¼šAå®šä½ â†’ Bé€‰é¢˜ â†’ Cæ–‡æ¡ˆ â†’ Dåˆ†é•œ")


# =========================
# ä¸»æµç¨‹
# =========================
agent_file = AGENTS[agent_name]
system_prompt = load_prompt(agent_file)
agent_id = agent_file.replace(".txt", "")  # agent_a / agent_b / agent_c / agent_d

llm = build_llm(model_name=model_name, temperature=temperature)

chat_key = f"chat::{user_id}::{agent_id}"
if chat_key not in st.session_state:
    raw = db_load_chat(user_id, agent_id)
    st.session_state[chat_key] = json_to_lc(raw)

chat = st.session_state[chat_key]

with st.expander("æŸ¥çœ‹å½“å‰ Agent çš„ System Promptï¼ˆåªè¯»ï¼‰", expanded=False):
    st.code(system_prompt)

# å±•ç¤ºå†å²å¯¹è¯
for msg in chat:
    if isinstance(msg, HumanMessage):
        with st.chat_message("user"):
            st.markdown(msg.content)
    else:
        with st.chat_message("assistant"):
            st.markdown(msg.content)

# ç”¨æˆ·è¾“å…¥
user_text = st.chat_input(f"æ­£åœ¨ä½¿ç”¨ï¼š{agent_name}ï¼ˆå¯ç²˜è´´é•¿æ–‡æœ¬ï¼‰")

if user_text:
    chat.append(HumanMessage(content=user_text))
    db_save_chat(user_id, agent_id, lc_to_json(chat))

    with st.chat_message("user"):
        st.markdown(user_text)

    rag_context = ""
    if use_rag:
        try:
            rag_context = retrieve_context(agent_id, user_text, k=topk, embed_model=embed_model)
        except Exception as e:
            st.error(f"RAG / Embedding åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
            st.stop()

    sys = system_prompt
    if rag_context:
        sys = (
            system_prompt
            + "\n\nã€å¯å¼•ç”¨çŸ¥è¯†åº“ç‰‡æ®µã€‘\n"
            + rag_context
            + "\n\nè¦æ±‚ï¼šå¦‚æœå¼•ç”¨äº†ç‰‡æ®µï¼Œè¯·åœ¨å›ç­”ä¸­æ ‡æ³¨æ¥æºç‰‡æ®µç¼–å·ã€‚"
        )

    messages = [SystemMessage(content=sys)] + chat

    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­â€¦"):
            resp = llm.invoke(messages)
            answer = extract_text(resp)
            st.markdown(answer)

    chat.append(AIMessage(content=answer))
    db_save_chat(user_id, agent_id, lc_to_json(chat))


with st.expander("ğŸ§ª è°ƒè¯•é¢æ¿", expanded=False):
    st.write("user_idï¼š", user_id)
    st.write("Agentï¼š", agent_name)
    st.write("agent_idï¼š", agent_id)
    st.write("LLMï¼š", model_name)
    st.write("Embeddingï¼š", embed_model)
    st.write("RAGï¼š", use_rag, "TopK=", topk)
    st.write("KB pathï¼š", str(KB_DIR / agent_id))
