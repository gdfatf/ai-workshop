from __future__ import annotations

from pathlib import Path
import os
from typing import Optional, List

import streamlit as st
from dotenv import load_dotenv

# ========= LangChain / Providers =========
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import OpenAIEmbeddings

# ========= LangChain core =========
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.documents import Document

# ========= RAG / Vector DB =========
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma

# ========= DOCX loader =========
from docx import Document as DocxDocument


# =========================
# Streamlit åŸºç¡€è®¾ç½®
# =========================
st.set_page_config(page_title="AI Workbench Â· Gemini + OpenAI RAG", layout="wide")
st.title("ğŸ§° AI Workbench Â· 13 Agents + RAGï¼ˆGemini LLM + OpenAI Embeddingï¼‰")


# =========================
# Env/Secrets
# =========================
load_dotenv()


def sget(key: str, default: Optional[str] = None) -> Optional[str]:
    """
    Streamlit Cloud: ä¼˜å…ˆ st.secrets
    æœ¬åœ°ï¼šå…œåº• os.getenvï¼ˆå·² load_dotenvï¼‰
    æ³¨æ„ï¼šæœ¬åœ°æ²¡æœ‰ secrets.toml æ—¶ï¼Œst.secrets çš„ contains ä¼šæŠ› FileNotFoundError
    """
    try:
        if hasattr(st, "secrets") and key in st.secrets:
            return str(st.secrets[key])
    except FileNotFoundError:
        pass
    return os.getenv(key, default)


GOOGLE_API_KEY = sget("GOOGLE_API_KEY")
OPENAI_API_KEY = sget("OPENAI_API_KEY")
APP_PASSWORD = sget("APP_PASSWORD", "")

GEMINI_MODEL_DEFAULT = sget("GEMINI_MODEL", "gemini-1.5-pro")  # ä½ ä¹Ÿå¯ä»¥æ”¹æˆ gemini-3-pro-*ï¼ˆä»¥ä½ è´¦å·å¯ç”¨ä¸ºå‡†ï¼‰
OPENAI_EMBED_MODEL_DEFAULT = sget("OPENAI_EMBED_MODEL", "text-embedding-3-large")

if not GOOGLE_API_KEY:
    st.error("ç¼ºå°‘ GOOGLE_API_KEYï¼šè¯·åœ¨ Streamlit Secrets æˆ–æœ¬åœ° .env ä¸­é…ç½®ã€‚")
    st.stop()

if not OPENAI_API_KEY:
    st.error("ç¼ºå°‘ OPENAI_API_KEYï¼šè¯·åœ¨ Streamlit Secrets æˆ–æœ¬åœ° .env ä¸­é…ç½®ï¼ˆç”¨äº Embedding / RAGï¼‰ã€‚")
    st.stop()


# =========================
# ç®€å•å¯†ç é—¨ï¼ˆå¯é€‰ï¼‰
# =========================
if APP_PASSWORD:
    if "authed" not in st.session_state:
        st.session_state.authed = False

    if not st.session_state.authed:
        st.subheader("ğŸ”’ è¯·è¾“å…¥è®¿é—®å¯†ç ")
        pwd = st.text_input("Password", type="password", key="pwd_input")
        if st.button("è¿›å…¥", key="pwd_btn"):
            if pwd == APP_PASSWORD:
                st.session_state.authed = True
                st.rerun()
            else:
                st.error("å¯†ç ä¸æ­£ç¡®")
        st.stop()


# =========================
# è·¯å¾„ä¸ç›®å½•
# =========================
PROMPTS_DIR = Path("agents") / "prompts"
KB_DIR = Path("kb")
DB_DIR = Path(".chroma_db")  # Streamlit Cloud: å®¹å™¨å†…ç›®å½•ï¼ˆé‡å¯å¯èƒ½ä¸¢ï¼Œä½†è¿è¡Œä¸­å¯ç”¨ï¼‰

PROMPTS_DIR.mkdir(parents=True, exist_ok=True)
KB_DIR.mkdir(parents=True, exist_ok=True)
DB_DIR.mkdir(parents=True, exist_ok=True)

AGENTS = {
    "01 äººè®¾å®šä½ Agent": "agent_01.txt",
    "02 åŸåˆ›çµæ„Ÿè®¨è®º Agent": "agent_02.txt",
    "03 çˆ†æ¬¾å†…å®¹ç­–åˆ’ Agent": "agent_03.txt",
    "04 æ–‡æ¡ˆèµŒæ³¨å¼ºåŒ– Agent": "agent_04.txt",
    "05 æ–‡æ¡ˆå¼€å¤´å¼ºåŒ– Agent": "agent_05.txt",
    "06 å†…å®¹äººæ ¼é­…åŠ›å¼ºåŒ– Agent": "agent_06.txt",
    "07 æ–‡æ¡ˆç”¨æˆ·éœ€æ±‚å¼ºåŒ– Agent": "agent_07.txt",
    "08 çº¿ç´¢è½¬åŒ–ç±»å†…å®¹ç­–åˆ’ Agent": "agent_08.txt",
    "09 å†…å®¹å½±åƒè®¾è®¡ Agent": "agent_09.txt",
    "10 å¤§çº²ç»“æ„ç»„ç»‡ Agent": "agent_10.txt",
    "11 å†…å®¹éª¨æ¶æ­å»º Agent": "agent_11.txt",
    "12 æ•´ä½“æ–‡æ¡ˆæ”¹å†™ Agent": "agent_12.txt",
    "13 ä¸ªäººIPè´¦å·è¿è¥é—®é¢˜è¯Šæ–­ Agent": "agent_13.txt",
}


# =========================
# å·¥å…·å‡½æ•°
# =========================
def load_prompt(filename: str) -> str:
    path = PROMPTS_DIR / filename
    if not path.exists():
        return "You are a helpful assistant."
    txt = path.read_text(encoding="utf-8").strip()
    return txt or "You are a helpful assistant."


def load_docx_text(path: Path) -> str:
    doc = DocxDocument(str(path))
    parts = [p.text for p in doc.paragraphs if p.text and p.text.strip()]
    return "\n".join(parts)


def load_kb_documents(agent_id: str) -> List[Document]:
    """
    ä» kb/agent_XX/ è¯»å– docx/txt
    """
    folder = KB_DIR / agent_id
    folder.mkdir(parents=True, exist_ok=True)

    docs: List[Document] = []
    for p in folder.rglob("*"):
        if p.is_dir():
            continue
        suf = p.suffix.lower()

        if suf == ".txt":
            docs.extend(TextLoader(str(p), encoding="utf-8").load())
        elif suf == ".docx":
            text = load_docx_text(p)
            if text.strip():
                docs.append(Document(page_content=text, metadata={"source": str(p)}))

    return docs


def extract_text(resp) -> str:
    """
    è§£å†³â€œä¹±ç â€ï¼šæœ‰æ—¶ resp.content æ˜¯ list[dict]ï¼ˆä¾‹å¦‚ [{'type':'text','text':'...'}]ï¼‰
    """
    content = getattr(resp, "content", resp)
    if isinstance(content, str):
        return content
    if isinstance(content, list):
        out = []
        for block in content:
            if isinstance(block, dict):
                out.append(str(block.get("text", "")))
            else:
                out.append(str(block))
        return "".join(out)
    return str(content)


def build_llm(model_name: str, temperature: float) -> ChatGoogleGenerativeAI:
    return ChatGoogleGenerativeAI(
        model=model_name,
        temperature=temperature,
        google_api_key=GOOGLE_API_KEY,
    )


def build_embeddings(model_name: str) -> OpenAIEmbeddings:
    # OpenAI Embeddings for RAG
    return OpenAIEmbeddings(
        model=model_name,
        api_key=OPENAI_API_KEY,
    )


@st.cache_resource(show_spinner=False)
def get_vectorstore(agent_id: str, embed_model: str):
    """
    æ¯ä¸ª agent ä¸€ä¸ª Chroma collection
    collection_name å¸¦ embed_modelï¼Œé¿å…ä½ åˆ‡ embedding æ¨¡å‹æ—¶æ—§åº“æ··ä¹±
    """
    embeddings = build_embeddings(embed_model)

    persist_dir = DB_DIR / agent_id
    persist_dir.mkdir(parents=True, exist_ok=True)

    safe_model = embed_model.replace("/", "_").replace(":", "_")
    collection_name = f"kb_{agent_id}__{safe_model}"

    vs = Chroma(
        collection_name=collection_name,
        embedding_function=embeddings,
        persist_directory=str(persist_dir),
    )

    # å¦‚æœç©ºåº“ï¼šå†™å…¥ kb
    try:
        existing = vs._collection.count()
    except Exception:
        existing = 0

    if existing == 0:
        raw_docs = load_kb_documents(agent_id)
        if raw_docs:
            splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=120)
            chunks = splitter.split_documents(raw_docs)
            vs.add_documents(chunks)
            vs.persist()

    return vs


def retrieve_context(agent_id: str, query: str, k: int, embed_model: str) -> str:
    vs = get_vectorstore(agent_id, embed_model=embed_model)
    docs = vs.similarity_search(query, k=k)
    if not docs:
        return ""

    blocks = []
    for i, d in enumerate(docs, 1):
        src = d.metadata.get("source", "kb")
        blocks.append(f"[ç‰‡æ®µ{i} | æ¥æº: {src}]\n{d.page_content}")
    return "\n\n".join(blocks)


# =========================
# Sidebar UI
# =========================
with st.sidebar:
    st.header("è®¾ç½®")

    st.caption("Keys æ£€æŸ¥")
    st.write("GOOGLE_API_KEY exists:", True)
    st.write("OPENAI_API_KEY exists:", True)

    st.divider()

    agent_name = st.selectbox("é€‰æ‹© Agent", list(AGENTS.keys()), key="agent_select")

    # Gemini æ¨¡å‹ï¼ˆä»¥ä½ è´¦å·å¯ç”¨ä¸ºå‡†ï¼›å¦‚æœ 3.0 pro çš„åå­—ä¸ä½ è´¦å·ä¸åŒï¼Œç›´æ¥æ”¹è¿™é‡Œå€™é€‰é¡¹ï¼‰
    gemini_candidates = [
        "gemini-1.5-pro",
        "gemini-1.5-flash",
        "gemini-3-pro-preview",
        "gemini-3-flash-preview",
    ]
    gemini_default = GEMINI_MODEL_DEFAULT if GEMINI_MODEL_DEFAULT in gemini_candidates else gemini_candidates[0]
    model_name = st.selectbox("LLMï¼ˆGeminiï¼‰", gemini_candidates, index=gemini_candidates.index(gemini_default), key="llm_select")

    # OpenAI Embedding æ¨¡å‹
    embed_candidates = [
        "text-embedding-3-large",
        "text-embedding-3-small",
    ]
    embed_default = OPENAI_EMBED_MODEL_DEFAULT if OPENAI_EMBED_MODEL_DEFAULT in embed_candidates else embed_candidates[0]
    embed_model = st.selectbox("Embeddingï¼ˆOpenAIï¼‰", embed_candidates, index=embed_candidates.index(embed_default), key="embed_select")

    temperature = st.slider("temperature", 0.0, 1.0, 0.3, 0.05, key="temp_slider")

    use_rag = st.toggle("å¯ç”¨ RAGï¼ˆä» kb æ£€ç´¢ï¼‰", value=True, key="rag_toggle")
    topk = st.slider("æ£€ç´¢ TopK", 1, 8, 4, 1, key="topk_slider")

    if st.button("æ¸…ç©ºå½“å‰ Agent å¯¹è¯", key="clear_chat_btn"):
        st.session_state.pop(f"chat::{agent_name}", None)
        st.rerun()

    st.divider()
    st.caption("ğŸ“Œ çŸ¥è¯†åº“ç›®å½•ï¼škb/agent_01 ~ kb/agent_13ï¼ˆdocx/txtï¼‰")
    st.caption("ğŸ“Œ Prompt ç›®å½•ï¼šagents/prompts/agent_01.txt ~ agent_13.txt")


# =========================
# ä¸»æµç¨‹
# =========================
agent_file = AGENTS[agent_name]
system_prompt = load_prompt(agent_file)
agent_id = agent_file.replace(".txt", "")  # agent_01 ... agent_13

llm = build_llm(model_name=model_name, temperature=temperature)

chat_key = f"chat::{agent_name}"
if chat_key not in st.session_state:
    st.session_state[chat_key] = []
chat = st.session_state[chat_key]

with st.expander("æŸ¥çœ‹å½“å‰ Agent çš„ System Promptï¼ˆåªè¯»ï¼‰", expanded=False):
    st.code(system_prompt)

# å±•ç¤ºå†å²å¯¹è¯
for msg in chat:
    if isinstance(msg, HumanMessage):
        with st.chat_message("user"):
            st.markdown(msg.content)
    else:
        with st.chat_message("assistant"):
            st.markdown(msg.content)

# ç”¨æˆ·è¾“å…¥
user_text = st.chat_input(f"æ­£åœ¨ä½¿ç”¨ï¼š{agent_name}ï¼ˆå¯ç²˜è´´é•¿æ–‡æœ¬ï¼‰")

if user_text:
    # 1) è®°å½•ç”¨æˆ·æ¶ˆæ¯
    chat.append(HumanMessage(content=user_text))
    with st.chat_message("user"):
        st.markdown(user_text)

    # 2) RAGï¼ˆå¼ºåˆ¶ç¨³å®šï¼šå¦‚æœ RAG æ‰“ä¸å¼€å°±ç›´æ¥æŠ¥é”™åœä¸‹ï¼Œé¿å…â€œçœ‹ä¼¼å¯ç”¨å…¶å®æ²¡æ£€ç´¢â€ï¼‰
    rag_context = ""
    if use_rag:
        try:
            rag_context = retrieve_context(agent_id, user_text, k=topk, embed_model=embed_model)
        except Exception as e:
            st.error(f"RAG / Embedding åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
            st.stop()

    # 3) system prompt æ‹¼è£…
    sys = system_prompt
    if rag_context:
        sys = (
            system_prompt
            + "\n\nã€å¯å¼•ç”¨çŸ¥è¯†åº“ç‰‡æ®µã€‘\n"
            + rag_context
            + "\n\nè¦æ±‚ï¼šå¦‚æœå¼•ç”¨äº†ç‰‡æ®µï¼Œè¯·åœ¨å›ç­”ä¸­æ ‡æ³¨æ¥æºç‰‡æ®µç¼–å·ã€‚"
        )

    messages = [SystemMessage(content=sys)] + chat

    # 4) è°ƒç”¨ LLM + æ˜¾ç¤ºå›å¤
    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­â€¦"):
            resp = llm.invoke(messages)
            answer = extract_text(resp)
            st.markdown(answer)

    # 5) è®°å½• assistant æ¶ˆæ¯
    chat.append(AIMessage(content=answer))


with st.expander("ğŸ§ª è°ƒè¯•é¢æ¿", expanded=False):
    st.write("Agentï¼š", agent_name)
    st.write("agent_idï¼š", agent_id)
    st.write("LLMï¼š", model_name)
    st.write("Embeddingï¼š", embed_model)
    st.write("RAGï¼š", use_rag, "TopK=", topk)
    st.write("KB pathï¼š", str(KB_DIR / agent_id))