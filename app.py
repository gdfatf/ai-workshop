from __future__ import annotations

from pathlib import Path
import os
from typing import Optional, List

import streamlit as st
from dotenv import load_dotenv

# LangChain + Gemini
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings

# LangChain core
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.documents import Document

# RAG / Vector DB
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma

# DOCX loader
from docx import Document as DocxDocument


# =========================
# Streamlit åŸºç¡€è®¾ç½®
# =========================
st.set_page_config(page_title="AI Workbench Â· Gemini", layout="wide")
st.title("ğŸ§° AI Workbench Â· 13 Agents + RAGï¼ˆGemini 3 Proï¼‰")


# =========================
# Secrets / Env è¯»å–
# =========================
load_dotenv()


def sget(key: str, default: Optional[str] = None) -> Optional[str]:
    """
    Streamlit Cloud: ä¼˜å…ˆ st.secrets
    æœ¬åœ°ï¼šå…œåº• os.getenvï¼ˆå·² load_dotenvï¼‰
    æ³¨æ„ï¼šæœ¬åœ°æ²¡æœ‰ secrets.toml æ—¶ï¼Œst.secrets çš„ __contains__ ä¼šæŠ› FileNotFoundError
    """
    try:
        if key in st.secrets:
            return str(st.secrets[key])
    except FileNotFoundError:
        pass

    return os.getenv(key, default)


GOOGLE_API_KEY = sget("GOOGLE_API_KEY")
GEMINI_MODEL_DEFAULT = sget("GEMINI_MODEL", "gemini-3-pro-preview")
APP_PASSWORD = sget("APP_PASSWORD", "")

if not GOOGLE_API_KEY:
    st.error("ç¼ºå°‘ GOOGLE_API_KEYï¼šè¯·åœ¨ Streamlit Cloud çš„ Secrets é‡Œé…ç½® GOOGLE_API_KEYã€‚")
    st.stop()


# =========================
# ç®€å•å¯†ç é—¨ï¼ˆå¯é€‰ï¼‰
# =========================
if APP_PASSWORD:
    if "authed" not in st.session_state:
        st.session_state.authed = False

    if not st.session_state.authed:
        st.subheader("ğŸ”’ è¯·è¾“å…¥è®¿é—®å¯†ç ")
        pwd = st.text_input("Password", type="password")
        if st.button("è¿›å…¥"):
            if pwd == APP_PASSWORD:
                st.session_state.authed = True
                st.rerun()
            else:
                st.error("å¯†ç ä¸æ­£ç¡®")
        st.stop()


# =========================
# è·¯å¾„ä¸ç›®å½•
# =========================
PROMPTS_DIR = Path("agents") / "prompts"
KB_DIR = Path("kb")
DB_DIR = Path(".chroma_db")

PROMPTS_DIR.mkdir(parents=True, exist_ok=True)
KB_DIR.mkdir(parents=True, exist_ok=True)
DB_DIR.mkdir(parents=True, exist_ok=True)

AGENTS = {
    "01 äººè®¾å®šä½ Agent": "agent_01.txt",
    "02 åŸåˆ›çµæ„Ÿè®¨è®º Agent": "agent_02.txt",
    "03 çˆ†æ¬¾å†…å®¹ç­–åˆ’ Agent": "agent_03.txt",
    "04 æ–‡æ¡ˆèµŒæ³¨å¼ºåŒ– Agent": "agent_04.txt",
    "05 æ–‡æ¡ˆå¼€å¤´å¼ºåŒ– Agent": "agent_05.txt",
    "06 å†…å®¹äººæ ¼é­…åŠ›å¼ºåŒ– Agent": "agent_06.txt",
    "07 æ–‡æ¡ˆç”¨æˆ·éœ€æ±‚å¼ºåŒ– Agent": "agent_07.txt",
    "08 çº¿ç´¢è½¬åŒ–ç±»å†…å®¹ç­–åˆ’ Agent": "agent_08.txt",
    "09 å†…å®¹å½±åƒè®¾è®¡ Agent": "agent_09.txt",
    "10 å¤§çº²ç»“æ„ç»„ç»‡ Agent": "agent_10.txt",
    "11 å†…å®¹éª¨æ¶æ­å»º Agent": "agent_11.txt",
    "12 æ•´ä½“æ–‡æ¡ˆæ”¹å†™ Agent": "agent_12.txt",
    "13 ä¸ªäººIPè´¦å·è¿è¥é—®é¢˜è¯Šæ–­ Agent": "agent_13.txt",
}


# =========================
# å·¥å…·å‡½æ•°
# =========================
def load_prompt(filename: str) -> str:
    path = PROMPTS_DIR / filename
    if not path.exists():
        return "You are a helpful assistant."
    txt = path.read_text(encoding="utf-8").strip()
    return txt or "You are a helpful assistant."


def load_docx_text(path: Path) -> str:
    doc = DocxDocument(str(path))
    parts = [p.text for p in doc.paragraphs if p.text.strip()]
    return "\n".join(parts)


def load_kb_documents(agent_id: str) -> List[Document]:
    """
    ä» kb/agent_XX/ è¯»å– docx/txt
    """
    folder = KB_DIR / agent_id
    folder.mkdir(parents=True, exist_ok=True)

    docs: List[Document] = []
    for p in folder.rglob("*"):
        if p.is_dir():
            continue
        suf = p.suffix.lower()

        if suf == ".txt":
            docs.extend(TextLoader(str(p), encoding="utf-8").load())

        elif suf == ".docx":
            text = load_docx_text(p)
            if text.strip():
                docs.append(Document(page_content=text, metadata={"source": str(p)}))

    return docs


def build_embeddings_or_none():
    """
    âœ… åªç”¨ text-embedding-004ï¼ˆä¸å†ç¢° embedding-001ï¼‰
    å¦‚æœè´¦å·/åœ°åŒºä¸æ”¯æŒï¼Œåˆ™è¿”å› None
    """
    try:
        emb = GoogleGenerativeAIEmbeddings(
            model="text-embedding-004",
            google_api_key=GOOGLE_API_KEY,
        )
        _ = emb.embed_query("ping")
        return emb
    except Exception:
        return None


@st.cache_resource(show_spinner=False)
def get_vectorstore(agent_id: str):
    """
    æ¯ä¸ª agent ä¸€ä¸ª Chroma collectionï¼ˆå¸¦ç‰ˆæœ¬å·é¿å…ç¼“å­˜/æ—§åº“å½±å“ï¼‰
    """
    embeddings = build_embeddings_or_none()
    if embeddings is None:
        # è®©ä¸Šå±‚å†³å®šæ˜¯å¦å¯ç”¨ RAG
        raise RuntimeError("Embedding ä¸å¯ç”¨ï¼štext-embedding-004 æ— æ³•è°ƒç”¨ï¼ˆæƒé™/åœ°åŒº/Key å¯èƒ½ä¸æ”¯æŒï¼‰ã€‚")

    persist_dir = DB_DIR / agent_id
    persist_dir.mkdir(parents=True, exist_ok=True)

    # âœ… collection_name åŠ  v2ï¼šé¿å…ä½ ä¹‹å‰ç”¨æ—§ embedding å»ºè¿‡åº“å¯¼è‡´æ··ä¹±
    vs = Chroma(
        collection_name=f"kb_{agent_id}_v2",
        embedding_function=embeddings,
        persist_directory=str(persist_dir),
    )

    # å¦‚æœç©ºåº“ï¼šå†™å…¥ kb
    try:
        existing = vs._collection.count()
    except Exception:
        existing = 0

    if existing == 0:
        raw_docs = load_kb_documents(agent_id)
        if raw_docs:
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=900,
                chunk_overlap=120,
            )
            chunks = splitter.split_documents(raw_docs)
            vs.add_documents(chunks)
            vs.persist()

    return vs


def retrieve_context(agent_id: str, query: str, k: int = 4) -> str:
    vs = get_vectorstore(agent_id)
    docs = vs.similarity_search(query, k=k)
    if not docs:
        return ""

    blocks = []
    for i, d in enumerate(docs, 1):
        src = d.metadata.get("source", "kb")
        blocks.append(f"[ç‰‡æ®µ{i} | æ¥æº: {src}]\n{d.page_content}")
    return "\n\n".join(blocks)


def build_llm(model_name: str, temperature: float):
    return ChatGoogleGenerativeAI(
        model=model_name,
        temperature=temperature,
        google_api_key=GOOGLE_API_KEY,
    )


def extract_text(resp) -> str:
    """
    è§£å†³â€œä¹±ç â€ï¼šGemini/LangChain æœ‰æ—¶è¿”å› resp.content æ˜¯ list[dict]
    """
    content = getattr(resp, "content", resp)
    if isinstance(content, str):
        return content
    if isinstance(content, list):
        out = []
        for block in content:
            if isinstance(block, dict):
                out.append(str(block.get("text", "")))
            else:
                out.append(str(block))
        return "".join(out)
    return str(content)


# =========================
# Sidebar UI
# =========================
with st.sidebar:
    st.header("è®¾ç½®")

    st.write("Gemini Key exists:", True)
    st.write("Default model:", GEMINI_MODEL_DEFAULT)

    with st.sidebar:
    st.header("è®¾ç½®")

    st.write("Gemini Key exists:", True)
    st.write("Gemini Model (default):", GEMINI_MODEL)

    # ===== Embedding è¯Šæ–­ï¼ˆä¸´æ—¶ï¼‰=====
    from langchain_google_genai import GoogleGenerativeAIEmbeddings

    st.divider()
    st.caption("ğŸ” Embedding å¯ç”¨æ€§è‡ªæ£€")

    try:
        emb = GoogleGenerativeAIEmbeddings(
            model="text-embedding-004",
            google_api_key=GOOGLE_API_KEY,
        )
        vec = emb.embed_query("ping")
        st.success(f"Embedding OK âœ… ç»´åº¦ = {len(vec)}")
    except Exception as e:
        st.error(f"Embedding FAILED âŒ {type(e).__name__}")
        st.code(str(e))

    agent_name = st.selectbox("é€‰æ‹© Agent", list(AGENTS.keys()))

    agent_name = st.selectbox("é€‰æ‹© Agent", list(AGENTS.keys()))

    model_candidates = [
        "gemini-3-pro-preview",
        "gemini-3-flash-preview",
        "gemini-1.5-pro",
        "gemini-1.5-flash",
    ]
    default_model = GEMINI_MODEL_DEFAULT if GEMINI_MODEL_DEFAULT in model_candidates else model_candidates[0]

    model_name = st.selectbox(
        "æ¨¡å‹",
        model_candidates,
        index=model_candidates.index(default_model),
    )

    temperature = st.slider("temperature", 0.0, 1.0, 0.3, 0.05)

    use_rag = st.toggle("å¯ç”¨ RAGï¼ˆä» kb æ£€ç´¢ï¼‰", value=True)
    topk = st.slider("æ£€ç´¢ TopK", 1, 8, 4, 1)

    # âœ… å¦‚æœ embedding ä¸å¯ç”¨ï¼Œè‡ªåŠ¨å…³é—­ RAGï¼Œé¿å…æ¯æ¬¡å¯¹è¯éƒ½ warning
    if use_rag:
        emb_check = build_embeddings_or_none()
        if emb_check is None:
            st.warning("å½“å‰è´¦å·æš‚ä¸å¯ç”¨ Embeddingï¼ˆtext-embedding-004 è°ƒç”¨å¤±è´¥ï¼‰ï¼ŒRAG å·²è‡ªåŠ¨å…³é—­ã€‚")
            use_rag = False

    if st.button("æ¸…ç©ºå½“å‰ Agent å¯¹è¯"):
        st.session_state.pop(f"chat::{agent_name}", None)
        st.rerun()

    st.divider()
    st.caption("ğŸ“Œ çŸ¥è¯†åº“ç›®å½•ï¼škb/agent_01 ~ kb/agent_13ï¼ˆdocx/txtï¼‰")
    st.caption("ğŸ“Œ Prompt ç›®å½•ï¼šagents/prompts/agent_01.txt ~ agent_13.txt")


# =========================
# ä¸»æµç¨‹
# =========================
agent_file = AGENTS[agent_name]
system_prompt = load_prompt(agent_file)
agent_id = agent_file.replace(".txt", "")  # agent_01 ... agent_13

llm = build_llm(model_name=model_name, temperature=temperature)

chat_key = f"chat::{agent_name}"
if chat_key not in st.session_state:
    st.session_state[chat_key] = []
chat = st.session_state[chat_key]

with st.expander("æŸ¥çœ‹å½“å‰ Agent çš„ System Promptï¼ˆåªè¯»ï¼‰", expanded=False):
    st.code(system_prompt)

# å±•ç¤ºå†å²å¯¹è¯
for msg in chat:
    if isinstance(msg, HumanMessage):
        with st.chat_message("user"):
            st.markdown(msg.content)
    else:
        with st.chat_message("assistant"):
            st.markdown(msg.content)

# ç”¨æˆ·è¾“å…¥
user_text = st.chat_input(f"æ­£åœ¨ä½¿ç”¨ï¼š{agent_name}ï¼ˆå¯ç²˜è´´é•¿æ–‡æœ¬ï¼‰")

if user_text:
    # 1ï¸âƒ£ è®°å½•ç”¨æˆ·æ¶ˆæ¯
    chat.append(HumanMessage(content=user_text))
    with st.chat_message("user"):
        st.markdown(user_text)

    # 2ï¸âƒ£ RAGï¼ˆå¯é€‰ï¼‰
    rag_context = ""
    if use_rag:
        try:
            rag_context = retrieve_context(agent_id, user_text, k=topk)
        except Exception as e:
            # è¿™é‡Œä¸å†åˆ·å±ï¼Œåªæç¤ºä¸€æ¬¡
            st.warning(f"RAG æš‚ä¸å¯ç”¨ï¼Œå·²è‡ªåŠ¨è·³è¿‡ã€‚åŸå› ï¼š{e}")
            rag_context = ""

    # 3ï¸âƒ£ system prompt æ‹¼è£…
    sys = system_prompt
    if rag_context:
        sys = (
            system_prompt
            + "\n\nã€å¯å¼•ç”¨çŸ¥è¯†åº“ç‰‡æ®µã€‘\n"
            + rag_context
            + "\n\nè¦æ±‚ï¼šå¦‚æœå¼•ç”¨äº†ç‰‡æ®µï¼Œè¯·åœ¨å›ç­”ä¸­æ ‡æ³¨æ¥æºç‰‡æ®µç¼–å·ã€‚"
        )

    messages = [SystemMessage(content=sys)] + chat

    # 4ï¸âƒ£ è°ƒç”¨ LLM + æ˜¾ç¤ºå›å¤
    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­â€¦"):
            resp = llm.invoke(messages)
            answer = extract_text(resp)
            st.markdown(answer)

    # 5ï¸âƒ£ è®°å½• assistant æ¶ˆæ¯
    chat.append(AIMessage(content=answer))


# =========================
# è°ƒè¯•é¢æ¿
# =========================
with st.expander("ğŸ§ª è°ƒè¯•é¢æ¿", expanded=False):
    st.write("å½“å‰ Agentï¼š", agent_name)
    st.write("agent_idï¼š", agent_id)
    st.write("æ¨¡å‹ï¼š", model_name)
    st.write("RAGï¼š", use_rag, "TopK=", topk)
    st.write("KB pathï¼š", str(KB_DIR / agent_id))